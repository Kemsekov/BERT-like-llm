{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with text files\n",
    "folder_with_text = 'harry potter/'\n",
    "\n",
    "# tokenizer vocabulary size\n",
    "vocab_size=4096\n",
    "\n",
    "# each line in a file will be converted to tokens and only first output_tokens will be used\n",
    "output_tokens=256\n",
    "\n",
    "# when training to restore skipped tokens, what is min and max amount\n",
    "# of skipped token per line\n",
    "min_skip_tokens=2\n",
    "max_skip_tokens=8\n",
    "\n",
    "# one token embedding\n",
    "emb_dim=256\n",
    "# one token embedding expansion in a model\n",
    "internal_dim=1024\n",
    "# how many transformer attention blocks to include in model\n",
    "attnetion_layers=10\n",
    "# how many heads for attention to use\n",
    "heads=16\n",
    "\n",
    "# where to save model checkpoints\n",
    "checkpoint_path = 'runs/harry-potter-linear'\n",
    "\n",
    "# number of epochs to train\n",
    "num_epochs=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Average token length 3.5714285714285716\n",
      "Encoded IDs: [242, 14, 356, 508, 236, 1912, 7]\n",
      "Encoded Tokens: ['Harry', ',', 'we', 'are', 'in', 'trouble', '!']\n",
      "Decoded Text: Harry , we are in trouble !\n"
     ]
    }
   ],
   "source": [
    "# for our dataset train tokenizer\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "import os\n",
    "\n",
    "# Initialize a tokenizer with a WordPiece model\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set up normalization and pre-tokenization\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=False,clean_text=False)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# [UNK] Represents any word or token that is not found in the model's vocabulary\n",
    "\n",
    "# [CLS] (Classification Token): Inserted at the beginning of every \n",
    "# input sequence. In classification tasks, the final hidden state corresponding \n",
    "# to this token is used as the aggregate sequence representation\n",
    "\n",
    "# [SEP] (Separator Token): Used to separate distinct sentences or segments within the input.\n",
    "# [PAD] (Padding Token): Used to pad input sequences to a uniform length, ensuring that batches of data have consistent dimensions.\n",
    "\n",
    "# [MASK] (Masking Token): Employed during the pre-training phase of models like BERT for masked language modeling. \n",
    "# Certain tokens in the input are replaced with [MASK], \n",
    "# and the model is trained to predict the original token, enabling it to learn bidirectional representations.\n",
    "\n",
    "# [END] (ending token): end of text token\n",
    "\n",
    "# Initialize a trainer with desired parameters\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=4,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\",\"[END]\",'[START]']\n",
    ")\n",
    "\n",
    "tokenizer_save_dir = os.path.join(folder_with_text,\"wordpiece_tokenizer.json\")\n",
    "txt_files = [folder_with_text+v for v in os.listdir(folder_with_text) if v.endswith(\".txt\")]\n",
    "txt_lines = \"\\n\".join([open(v).read() for v in txt_files])\n",
    "\n",
    "# Train and save the tokenizer on your corpus \n",
    "tokenizer.train(txt_files, trainer)\n",
    "tokenizer.save(tokenizer_save_dir)\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_save_dir)\n",
    "vocab_size=tokenizer.get_vocab_size()\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Harry, we are in trouble!\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(\"Average token length\",len(sample_text)/len(encoded.tokens))\n",
    "print(\"Encoded IDs:\", encoded.ids)\n",
    "print(\"Encoded Tokens:\", encoded.tokens)\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "print(\"Decoded Text:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length analysis\n",
      "text lines\t 34517\n",
      "line chars mean\t 177.687\n",
      "line chars std\t 240.073\n",
      "0.05 quantile\t 39.0\n",
      "0.95 quantile\t 473.0\n",
      "mean token len\t 3.930851187856247\n"
     ]
    }
   ],
   "source": [
    "# analyze dataset and tokenizer\n",
    "import numpy as np\n",
    "txt_split = txt_lines.split(\"\\n\")\n",
    "txt_split = [v for v in txt_split if len(v)>30]\n",
    "lengths = [len(v) for v in txt_split]\n",
    "lengths=np.array(lengths)\n",
    "longest = lengths.argsort()[-100:]\n",
    "\n",
    "longest_texts = [txt_split[v] for v in longest]\n",
    "longest_texts_emb_token_length = [len(v)/len(tokenizer.encode(v).tokens) for v in longest_texts]\n",
    "mean_token_length = np.array(longest_texts_emb_token_length).mean()\n",
    "\n",
    "print(\"Text length analysis\")\n",
    "print(\"text lines\\t\",len(txt_split))\n",
    "print(\"line chars mean\\t\",lengths.mean().round(3))\n",
    "print(\"line chars std\\t\",lengths.std().round(3))\n",
    "print(\"0.05 quantile\\t\",np.quantile(lengths,0.05))\n",
    "print(\"0.95 quantile\\t\",np.quantile(lengths,0.95))\n",
    "# mean amount of chars per token\n",
    "print('mean token len\\t',mean_token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33826, 691)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from kemsekov_torch.train import split_dataset\n",
    "import torch\n",
    "\n",
    "class TokenDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, text_lines,pad_token = '[PAD]',mask_token = '[MASK]',start_token='[START]',end_token='[END]', output_tokens = 1024,min_skip_tokens=2,max_skip_tokens = 10):\n",
    "        super().__init__()\n",
    "        self.text = text_lines\n",
    "        self.pad_token = tokenizer.encode(pad_token).ids[0]\n",
    "        self.mask_token = tokenizer.encode(mask_token).ids[0]\n",
    "        self.start_token = tokenizer.encode(start_token).ids[0]\n",
    "        self.end_token = tokenizer.encode(end_token).ids[0]\n",
    "        self.output_tokens=output_tokens\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_skip_tokens=min_skip_tokens\n",
    "        self.max_skip_tokens=max_skip_tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        ids_orig = tokenizer.encode(text).ids\n",
    "        ids=torch.tensor(([self.start_token]+ids_orig+[self.pad_token]*(self.output_tokens))[:self.output_tokens-1]+[self.end_token])\n",
    "        \n",
    "        to_skip = random.randint(self.min_skip_tokens,self.max_skip_tokens)\n",
    "        to_skip = min(to_skip,len(ids_orig)//4)\n",
    "        \n",
    "        return self.neighbor_middle_token_pair(ids,to_skip)\n",
    "    \n",
    "    def neighbor_middle_token_pair(self,tokens,skip_tokens=4):\n",
    "        aval_id = torch.where(tokens!=self.pad_token)[0]\n",
    "        \n",
    "        skipped = tokens.clone()\n",
    "        ind = torch.rand(aval_id.shape).argsort()[:skip_tokens]\n",
    "        \n",
    "        skipped[aval_id[ind]]=self.mask_token\n",
    "        return skipped,tokens\n",
    "\n",
    "dataset = TokenDataset(\n",
    "    tokenizer,\n",
    "    txt_split,\n",
    "    pad_token='[PAD]',\n",
    "    mask_token='[MASK]',\n",
    "    output_tokens=output_tokens,\n",
    "    min_skip_tokens=min_skip_tokens,\n",
    "    max_skip_tokens=max_skip_tokens\n",
    ")\n",
    "\n",
    "train_dataset,test_dataset,train_loader, test_loader = split_dataset(dataset,test_size=0.02,batch_size=16,random_state=None)\n",
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=True\n",
      "skipped at tensor([], dtype=torch.int64)\n",
      "[START] The idea that Dumbledore valued his opinion this highly made Harry feel even more deeply ashamed that he had failed in the task of retrieving the Horcrux memory , and he shifted guiltily in his seat as Dumbledore raised the first of the two bottles to the light and examined it .                                                                                                                                                                                             [END]\n",
      "[MASK] The idea that [MASK] valued his opinion this highly made Harry [MASK] even [MASK] deeply ashamed that he had failed in the task of retrieving the Horcrux memory , [MASK] he shifted guiltily in his seat as Dumbledore raised the first of the [MASK] bottles to the [MASK] and examined [MASK] .                                                                                                                                                                                             [END]\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=True\n",
    "import random\n",
    "ind = random.randint(0,len(train_dataset)-1)\n",
    "skip,neigh = dataset[ind]\n",
    "\n",
    "i = random.randint(0,len(neigh)-1)\n",
    "print(\"skipped at\",torch.where(neigh==dataset.mask_token)[0])\n",
    "neigh_text = tokenizer.decode(neigh.tolist(),skip_special_tokens=False).replace(\" ##\",\"\").replace('[PAD]','')\n",
    "skip_text = tokenizer.decode(skip.tolist(),skip_special_tokens=False).replace(\" ##\",\"\").replace('[PAD]','')\n",
    "\n",
    "print(neigh_text)\n",
    "print(skip_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 145M parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256, 4096]), torch.Size([256]), torch.Size([256]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kemsekov_torch.residual import ResidualBlock, Residual\n",
    "from kemsekov_torch.attention import LinearSelfAttentionBlock, TransformerSelfAttentionBlock\n",
    "from kemsekov_torch.common_modules import *\n",
    "from kemsekov_torch.positional_emb import ConcatPositionalEmbeddingPermute\n",
    "from kemsekov_torch.rotary_emb import RotaryEmbInplace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Module for token to embedding vector learning\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(vocab_size, embedding_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(embedding_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    #normal init\n",
    "    def reset_parameters(self):\n",
    "        # Initialize weights with a normal distribution\n",
    "        std = 1.0 / (self.vocab_size**0.5)\n",
    "        nn.init.normal_(self.weight, mean=0.0, std=std)\n",
    "        # Initialize bias to zeros\n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Input is expected to be a tensor of indices\n",
    "        output = torch.nn.functional.embedding(input, self.weight) + self.bias\n",
    "        return output.transpose(-1,-2)\n",
    "class SimpleTransformer(torch.nn.Module):\n",
    "    def __init__(self,emb_dim=256,internal_dim=512, attnetion_layers = 10,heads=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        attn_common = dict(\n",
    "            input_dim=internal_dim,\n",
    "            mlp_dim=internal_dim*4,\n",
    "            heads=heads,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        \n",
    "        self.emb = Embedding(vocab_size,emb_dim)\n",
    "        self.attention = torch.nn.Sequential(\n",
    "            # expand dimensions\n",
    "            ResidualBlock(\n",
    "                emb_dim,\n",
    "                internal_dim,\n",
    "                kernel_size=1,\n",
    "                dimensions=1\n",
    "            ),\n",
    "            \n",
    "            Residual([\n",
    "                # use RoPE as first module\n",
    "                RotaryEmbInplace(internal_dim),\n",
    "                # then stack linear SA blocks\n",
    "                FlattenSpatialDimensions([\n",
    "                    LinearSelfAttentionBlock(**attn_common)\n",
    "                    # TransformerSelfAttentionBlock(attn_common['input_dim'],attn_common['heads'],attn_common['mlp_dim'],batch_first=True)\n",
    "                    for i in range(attnetion_layers)\n",
    "                ])\n",
    "            ])\n",
    "        )\n",
    "        # map emb dim to token\n",
    "        self.fc = torch.nn.Linear(internal_dim,vocab_size)\n",
    "    def forward(self,x : torch.Tensor):\n",
    "        x = self.emb(x)\n",
    "        x = self.attention(x)\n",
    "        x=x.transpose(-1,-2)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleTransformer(\n",
    "    emb_dim=emb_dim,\n",
    "    internal_dim=internal_dim,\n",
    "    attnetion_layers=attnetion_layers,\n",
    "    heads=heads\n",
    ")\n",
    "\n",
    "print(f\"model {sum([v.numel() for v in model.parameters()])//1000//1000}M parameters\")\n",
    "model(neigh[None,:]).shape,neigh.shape,skip.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters 145.57 M\n",
      "loaded training state from runs/harry-potter-linear/last/state\n",
      "trying to capture model architecture...\n",
      "Saved model architecture at runs/harry-potter-linear/model.pt. You can torch.load it and update it's weights with checkpoint\n",
      "\n",
      "Epoch 472/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 0:   2%|▏         | 43/2115 [00:15<05:37,  6.15it/s, f1=0.9008, f1 skip=0.2561, general_loss=0.0676, loss=4.0390, skipped_tokens_loss=3.9714] "
     ]
    }
   ],
   "source": [
    "from kemsekov_torch.train import train\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "CE = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "f1__ = MulticlassF1Score(vocab_size)\n",
    "def f1(x,y):\n",
    "    return f1__(x.detach().cpu(),y.detach().cpu())\n",
    "\n",
    "def compute_loss_and_metric(model,batch):\n",
    "    skip,neigh = batch[0],batch[1]\n",
    "\n",
    "    neigh_pred = model(skip)\n",
    "    neigh_pred=neigh_pred.view(-1,neigh_pred.shape[-1])\n",
    "    neigh = neigh.view(-1)\n",
    "    \n",
    "    general_loss = CE(neigh_pred,neigh)\n",
    "    \n",
    "    skip_ind = skip.view(-1)==dataset.mask_token\n",
    "    skip_pred = neigh_pred[skip_ind]\n",
    "    skip_true = neigh[skip_ind]\n",
    "    skipped_tokens_loss = CE(skip_pred,skip_true)\n",
    "    \n",
    "    # use two losses, to enforce model not only reconstruct back original\n",
    "    # input sequence, but also pay same amount of attention to skipped tokens\n",
    "    return general_loss+skipped_tokens_loss,{\n",
    "        'f1': f1(neigh_pred,neigh),\n",
    "        'f1 skip': f1(skip_pred,skip_true),\n",
    "        'general_loss':general_loss,\n",
    "        'skipped_tokens_loss':skipped_tokens_loss\n",
    "    }\n",
    "\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(),0.001,betas=(0.9, 0.95))\n",
    "sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,len(train_loader)*10)\n",
    "\n",
    "_ = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    compute_loss_and_metric,\n",
    "    checkpoint_path,\n",
    "    f'{checkpoint_path}/last',\n",
    "    gradient_clipping_max_norm=1,\n",
    "    accelerate_args={\n",
    "        # 'gradient_accumulation_steps':8,\n",
    "        'mixed_precision':'bf16',\n",
    "        'dynamo_backend':'inductor'\n",
    "    },\n",
    "    save_on_metric_improve=['f1 skip'],\n",
    "    optimizer=optim,\n",
    "    num_epochs=num_epochs,\n",
    "    scheduler=sch,\n",
    "    checkpoints_count=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading runs/harry-potter-linear/checkpoints/epoch-400/state\n"
     ]
    }
   ],
   "source": [
    "from kemsekov_torch.train import load_checkpoint, load_last_checkpoint\n",
    "\n",
    "# load model\n",
    "model = torch.jit.load(f\"{checkpoint_path}/model.pt\")\n",
    "model = load_checkpoint(model,checkpoint_path,-1).cpu().eval().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] It took Harry a few moments to realize what McLaggen was talking about .                                                                                                                                                                                                                                                 [END]\n",
      "[START] [MASK] [MASK] Harry [MASK] few moments to realize what McLaggen was talking about .                                                                                                                                                                                                                                                 [END]\n",
      "[START] It was Harry a few moments to realize what McLaggen was talking about .                                                                                                                                                                                                                                                 [END]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9216)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = test_dataset.dataset\n",
    "ind = random.randint(0,len(d)-1)\n",
    "skipped_t,true_t = d[ind]\n",
    "\n",
    "pred = model(skipped_t[None,:])[0]\n",
    "pred_tokens = pred.softmax(-1).argmax(-1)\n",
    "\n",
    "true_text = tokenizer.decode(true_t.tolist(),skip_special_tokens=False).replace(\" ##\",\"\").replace('[PAD]','')\n",
    "skip_text = tokenizer.decode(skipped_t.tolist(),skip_special_tokens=False).replace(\" ##\",\"\").replace('[PAD]','')\n",
    "pred_text = tokenizer.decode(pred_tokens.tolist(),skip_special_tokens=False).replace(\" ##\",\"\").replace('[PAD]','')\n",
    "\n",
    "print(true_text)\n",
    "print(skip_text)\n",
    "print(pred_text)\n",
    "\n",
    "f1(pred,true_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
